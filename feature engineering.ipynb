{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering\n",
        "\n",
        "##Assignment\n",
        "\n",
        "**Question. No. 1. What is a parameter?**\n",
        "\n",
        "**Answer.** In statistics and machine learning, a parameter is a value that characterizes a model or population. For example, in a linear regression model y=mx+c, both m (slope) and c (intercept) are parameters that define the line's shape. In machine learning, parameters are learned from data during training. These include weights in neural networks or coefficients in regression, which help the model make accurate predictions.\n",
        "\n",
        "**Question. No. 2. What is correlation? What does negative correlation  mean?**\n",
        "\n",
        "**Answer.** Correlation measures the strength and direction of a linear relationship between two variables. It ranges from -1 to +1. A value near +1 indicates a strong positive relationship, where both variables increase together. A negative correlation (value close to -1) means that as one variable increases, the other decreases. For example, the number of hours a person spends watching TV may be negatively correlated with academic performance—more TV might mean lower grades.For example, as outdoor temperature rises, heating expenses typicaly fall.\n",
        "\n",
        "**Question. No. 3. Define Machine Learning. What are the main components in Machine Learning?**\n",
        "\n",
        "**Answer.** Machine learning is a subfield of artificial intelligence that enables computers to learn patterns from data and make decisions or predictions without being explicitly programmed. The main components of machine learning are:\n",
        "\n",
        "**Model** - a mathematical structure used to make predictions.\n",
        "\n",
        "**Loss Function** - a way to measure how good the model is.\n",
        "\n",
        "**Optimizer** - adjusts model parameters to reduce error.\n",
        "\n",
        "**Data** - labeled or unlabeled examples that the model learns from.        vjkgk\n",
        "\n",
        "**Question. No. 4. How does loss value help in determining whether the model is good or not?**\n",
        "\n",
        "**Answer.** The loss value is a measure of how far off a model's predictions are from the actual results. A lower loss indicates that the model is making accurate predictions. It helps during training by guiding the optimizer on how to adjust parameters to improve the model. However, a very low loss on training data can sometimes indicate overfitting, where the model performs well on known data but poorly on new, unseen data.\n",
        "\n",
        "**Question. No. 5. What are continuous and categorical variables?**\n",
        "\n",
        "**Answer.** Continuous variables are numerical and can take an infinite number of values within a range, such as height, weight, or temperature. They are measurable quantities. Categorical variables represent distinct categories or groups and are not inherently numerical. Examples include gender, country, or type of product. In machine learning, it’s important to treat them differently—for instance, continuous variables might be scaled, while categorical variables are encoded into numerical format.\n",
        "\n",
        "**Question. No. 6. How do we handle categorical variable in Machine Learning? What are the common techniques?**\n",
        "\n",
        "**Answer.** Categorical variables must be converted into a numerical form before being used in most machine learning models. Common techniques include Label Encoding, where each category is assigned a unique integer (e.g., Male = 0, Female = 1), and One-Hot Encoding, which creates binary columns for each category. Another method is Target Encoding, where categories are replaced with their average target value. Choosing the right encoding depends on the type of data and the algorithm used.\n",
        "\n",
        "**Question. No. 7. What do you mean by training and testing a dataset?**\n",
        "\n",
        "**Answer.** Training a dataset involves feeding a machine learning model with input features and known outcomes so it can learn patterns. Testing a dataset means using new, unseen data to evaluate the model’s performance. Splitting the dataset ensures that the model doesn’t just memorize the training data but can also generalize to new inputs. This practice helps detect overfitting and validates how well the model might perform in real-world scenarios.\n",
        "\n",
        "\n",
        "**Question. No. 8. What is sklearn.preprocessing?**\n",
        "\n",
        "**Answer.** sklearn.preprocessing is a module in the scikit-learn library that contains tools for preparing data before model training. It includes methods for feature scaling (like StandardScaler, MinMaxScaler), normalization, encoding categorical variables (like OneHotEncoder, LabelEncoder), and more. Preprocessing ensures that features are on a similar scale and in a suitable format for the learning algorithms, improving training speed and accuracy.\n",
        "\n",
        "**Question. No. 9. What is a Test set?**\n",
        "\n",
        "**Answer.** A test set is a separate portion of the dataset reserved to evaluate the trained model’s performance. It is not used during the training phase, which helps in assessing how well the model generalizes to unseen data. For example, in a typical 80/20 split, 80% of the data is used for training, while 20% is used for testing. A good model should perform well on both training and test sets to be considered robust.\n",
        "\n",
        "**Question. No. 10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?**\n",
        "\n",
        "**Answer.** In Python, we commonly use train_test_split() from sklearn.model_selection to divide the dataset. A typical split is 70–80% for training and 20–30% for testing. To approach a machine learning problem:\n",
        "\n",
        "1. Understand the problem and data.\n",
        "\n",
        "2. Perform EDA (exploratory data analysis).\n",
        "\n",
        "3. Preprocess the data (handle missing values, scale, encode).\n",
        "\n",
        "4. Split the data.\n",
        "\n",
        "5. Choose and train a model.\n",
        "\n",
        "6. Evaluate the model.\n",
        "\n",
        "7. Fine-tune and deploy.\n",
        "\n",
        "**Question. No. 11. Why do we have to perform EDA before fitting a model to the data?**\n",
        "\n",
        "**Answer.** EDA, or Exploratory Data Analysis, is crucial for understanding the data’s structure, patterns, and anomalies. It helps identify missing values, outliers, feature distributions, and relationships between variables. By performing EDA, you can make informed decisions on preprocessing, feature selection, or engineering. It also reveals whether your assumptions about the data hold, which directly affects model performance. Skipping EDA can lead to incorrect models due to unseen data issues.\n",
        "\n",
        "**Question. No. 12. What is correlation?**\n",
        "\n",
        "**Answer.** Correlation is a statistical measure that describes the degree to which two variables move in relation to each other. It ranges from -1 to +1. A correlation of +1 means both variables increase together perfectly, while -1 means one increases as the other decreases. A correlation of 0 means no linear relationship. It’s widely used in data analysis and machine learning to detect dependencies between variables and guide feature selection.\n",
        "\n",
        "**Question. No. 13. What does negative correlation mean?**\n",
        "\n",
        "**Answer.** Negative correlation means that as one variable increases, the other decreases. For example, if hours spent watching TV increase, academic performance might decline—this shows an inverse relationship. In mathematical terms, negative correlation values range between 0 and -1. A value of -1 means a perfect negative correlation. This insight helps in understanding which features might be inversely affecting the target variable in a model.\n",
        "\n",
        "**Question. No. 14. How can you find correlation between variables in Python?**\n",
        "\n",
        "**Answer.** In Python, correlation can be computed using pandas.DataFrame.corr(), which returns a correlation matrix using Pearson's method by default. You can also use scipy.stats.pearsonr() for more detailed output. To visualize correlation, the seaborn library’s heatmap() is commonly used, which displays a color-coded matrix showing how strongly each variable relates to the others. This helps in selecting important features and avoiding multicollinearity.\n",
        "\n",
        "**Question. No. 15. What is causation? Explain difference between correlation and causation with an example.**\n",
        "\n",
        "**Answer.** Causation implies a direct cause-and-effect relationship between two variables, whereas correlation simply shows they are related. For example, increased ice cream sales correlate with higher drowning incidents, but ice cream does not cause drowning. The real cause might be summer (a lurking variable). In machine learning and statistics, it's essential not to confuse correlation with causation, as it can lead to incorrect assumptions and decisions in analysis and model design.\n",
        "\n",
        "**Question. No. 16. What is Optimizer? What are different types of optimizers? Explain each with an example.**\n",
        "\n",
        "**Answer.** An optimizer is an algorithm used during model training to adjust parameters like weights and biases to minimize the loss function. Common optimizers include:\n",
        "\n",
        "**Gradient Descent**: Updates parameters using the full dataset.\n",
        "\n",
        "**Stochastic Gradient Descent (SGD)**: Updates parameters using one data point at a time, faster but noisier.\n",
        "\n",
        "**Adam:** Combines momentum and adaptive learning rate, widely used for deep learning.\n",
        "For example, Adam is often used in training neural networks for its efficiency and speed.\n",
        "\n",
        "**Question. No. 17. What is sklearn.linear_model?**\n",
        "\n",
        "**Answer.** sklearn.linear_model is a module in the Scikit-learn library that provides linear models for regression and classification. Some key classes include LinearRegression for predicting continuous values, LogisticRegression for binary classification, and Ridge and Lasso for regularized regression. These models are simple yet powerful, easy to interpret, and often serve as a good starting point for many machine learning tasks, especially when relationships between variables are linear.\n",
        "\n",
        "**Question. No. 18. What does model.fit() do? What arguments must be given?**\n",
        "\n",
        "**Answer.** The model.fit() function trains the model on the given data. It takes two primary arguments: X (features/input variables) and y (target/output variable). During this process, the model learns the patterns and relationships in the data by adjusting its internal parameters. Some models may accept additional arguments such as epochs, batch_size, or validation_data depending on the algorithm and library used (especially in deep learning frameworks like Keras).\n",
        "\n",
        "**Question. No. 19. What does model.predict() do? What arguments must be given?**\n",
        "\n",
        "**Answer.** The model.predict() function is used after training to make predictions on new or unseen data. The only required argument is X_new, which is the new input data for which we want to make predictions. The function outputs predicted values in the same format as the target variable. For example, if you're predicting house prices, model.predict(X_test) will return the predicted prices for the test dataset.\n",
        "\n",
        "**Question. No. 20. What are continuous and categorical variables?**\n",
        "\n",
        "**Answer.** Continuous variables are numeric variables that can take an infinite number of values within a range, like temperature, height, or salary. They are measurable and often used in regression problems. Categorical variables represent discrete groups or classes, such as gender, color, or education level. These can be nominal (no order) or ordinal (with order). Proper handling of both types is essential during preprocessing for accurate machine learning model performance.\n",
        "\n",
        "**Question. No. 21. What is feature scaling? How does it help in Machine Learning?**\n",
        "\n",
        "**Answer.** Feature scaling is the process of normalizing or standardizing the range of independent variables or features. It ensures that no feature dominates others due to differing units or scales. For example, age (0–100) and salary (in lakhs) need to be scaled for algorithms like KNN or SVM to function properly. Common scaling methods include Min-Max scaling (0 to 1) and Standardization (mean = 0, std = 1). It speeds up training and improves model accuracy.\n",
        "\n",
        "**Question. No. 22. How do we perform scaling in Python?**\n",
        "\n",
        "**Answer.** In Python, we can use the sklearn.preprocessing module for scaling. The two most commonly used classes are:\n",
        "\n",
        "**StandardScaler:** Scales data to have a mean of 0 and standard deviation of 1.\n",
        "\n",
        "**MinMaxScaler:** Scales features to a specific range, usually [0,1].\n",
        "Usage example:\n"
      ],
      "metadata": {
        "id": "_xY5BWJPlLJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Original data:\\n\", X)\n",
        "print(\"\\nScaled data:\\n\", X_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRsnzXT2129d",
        "outputId": "b5bc0685-e362-4beb-b3af-022e88226ae9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data:\n",
            " [[1 2]\n",
            " [3 4]\n",
            " [5 6]]\n",
            "\n",
            "Scaled data:\n",
            " [[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These tools help ensure balanced input for the model.\n",
        "\n",
        "**Question. No. 23. What is sklearn.preprocessing?**\n",
        "\n",
        "**Answer.** sklearn.preprocessing is a module in the Scikit-learn library that provides tools for transforming and preparing raw data before modeling. It includes techniques for:\n",
        "\n",
        "**Scaling** (StandardScaler, MinMaxScaler)\n",
        "\n",
        "**Encoding categorical variables** (LabelEncoder, OneHotEncoder)\n",
        "\n",
        "**Binarizing features**\n",
        "\n",
        "**Polynomial feature generation**, and more.\n",
        "Proper preprocessing with this module ensures better learning efficiency, faster convergence, and improved model performance, especially with algorithms sensitive to feature distributions.\n",
        "\n",
        "**Question. No. 24. How do we split data for model fitting (training and testing) in Python?**\n",
        "\n",
        "**Answer.** We use train_test_split() from sklearn.model_selection to divide the dataset into training and testing sets. A common split is 70% training and 30% testing. Example:\n"
      ],
      "metadata": {
        "id": "5P4_ALDy2Ctu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14], [15, 16], [17, 18], [19, 20]])\n",
        "y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1]) # Sample target variable\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"Original X data:\\n\", X)\n",
        "print(\"\\nOriginal y data:\\n\", y)\n",
        "print(\"\\nX_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pAF54eg2m39",
        "outputId": "8833911d-d06f-441e-ea51-9627359de8c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original X data:\n",
            " [[ 1  2]\n",
            " [ 3  4]\n",
            " [ 5  6]\n",
            " [ 7  8]\n",
            " [ 9 10]\n",
            " [11 12]\n",
            " [13 14]\n",
            " [15 16]\n",
            " [17 18]\n",
            " [19 20]]\n",
            "\n",
            "Original y data:\n",
            " [0 1 0 1 0 1 0 1 0 1]\n",
            "\n",
            "X_train shape: (7, 2)\n",
            "X_test shape: (3, 2)\n",
            "y_train shape: (7,)\n",
            "y_test shape: (3,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method helps evaluate how well a model generalizes to new data and is a critical step in the machine learning pipeline.\n",
        "\n",
        "**Question. No. 25. Explain data encoding?**\n",
        "\n",
        "**Answer.** Data encoding transforms categorical values into a numerical format suitable for machine learning algorithms. This is necessary because most models require numeric input. Techniques include:\n",
        "\n",
        "**Label Encoding:** Assigns an integer to each category.\n",
        "\n",
        "**One-Hot Encoding:** Converts categories into binary vectors.\n",
        "\n",
        "**Target Encoding:** Replaces categories with average target values.\n",
        "Proper encoding prevents misleading the model with ordinal relationships where none exist and ensures better training performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "ox6WUmEe2zdg"
      }
    }
  ]
}